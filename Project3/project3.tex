% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
amsmath,amssymb,
aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{subfiles}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{float}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{physics}
\usepackage{dsfont}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{tensor}
\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\newcommand{\Hp}{\mathcal{H}}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}
\renewcommand{\figurename}{Fig.}
\renewcommand{\tablename}{Table}
\makeatletter
\renewcommand{\subsubsection}{%
	\@startsection
	{subsubsection}%
	{3}%
	{\z@}%
	{.8cm \@plus1ex \@minus .2ex}%
	{.5cm}%
	{\normalfont\small\centering}%
}
\makeatother
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\p}{\partial}



\begin{document}
	
\title{Project 3}
\author{Edvard B. RÃ¸rnes}
\email{e.b.rornes@fys.uio.no}
\author{Isak O. Rukan}
\email{Insert Email}
\affiliation{Institute of Physics, University of Oslo,\\0371 Oslo,  Norway}
\date{\today}

\begin{abstract}
	Abstracting very cool
\end{abstract}

\maketitle
\tableofcontents

\section{Introduction}


\section{Theory}
\subsection{Gravitational Waves}
% Gravitational waves are created by the merging of large bodies, typically with masses of order \(\sim 50\) times that of the sun. 
\subsubsection{}

\subsection{Recurrent Neural Networks}
The Recurrent Neural Networks (RNNs) are a class of neural networks specifically designed to handle sequential data or data with temporal dependencies. Unlike traditional FeedForward Neural Networks, RNNs are capable of "remembering" information from previous time steps. This is done through the so called `hidden state', which acts as a form of memory by retaining information about prior computations. The hidden state is essentially an array of data that is updated at each time step based on the input data and the previous hidden state. Alhough this enables RNN to access the temporal dependencies of the data at hand, it greatly increases the commputation time compared to that of the FFNN. The standard RNN consists of only one hidden layer, but it is certainly possible to have more than one hidden layer. In fact, this is commonly referred to as the stacked RNN (SRNN), and we will arrive at this neural network further down. However, firstly, we present the structure and general algorithm for the RNN.

\subsubsection{Structure}
The RNN processes input sequentially, with information flowing step-by-step from the input to the output. This is done with the introduction of a hidden state \(h_{t}\), where the subscript denotes at time \(t\). The network can be summarized by the following two equations \cite{tallec2017unbiasingtruncatedbackpropagationtime}:
\begin{subequations}
\begin{align}
	h_{t} &= \sigma^{(h)}\left(W_{hx}X_{t} + W_{hh}h_{t-1}  + b_{h}\right), \label{eq:hidden_state_RNN} \\
	\tilde{y}_{t} &= \sigma^{(\text{out})}\left(W_{yh}h_{t} + b_{y}\right).	\label{eq:output_RNN}
\end{align}
\end{subequations}
Here, \(\sigma_h\) and \(\sigma_{\text{out}}\) is the activation function for the hidden layer and the output layer respectively. \(W_{xh}\) is the weight from input to hidden layer, \(W_{hh}\) the hidden layer, \(W_{yh}\) the output layer and \(\tilde{y}\) the output of the RNN. Let now \(t\) be divided into a discrete set of times \((t_i)_{i\in N}\). Substituting \eqref{eq:hidden_state_RNN} into itself recursively leads to a formula for computing \(h_{t_n}\):
\begin{align}	\label{eq:hidden_state_computation_RNN}
	h_{t_n} = \sigma^{(h)}\biggl( W_{hx}X_{t_n} &+ W_{hh}\sigma_h \biggl( W_{hx}X_{t_{n-1}} \notag\\
	&+ W_{hh}\sigma_h \left(\dots + b_h\right) + b_h\biggr) 
	+b_h\biggr)
\end{align}
This shows that the hidden state at time \(t_n\) is dependent on the input \(X_{t}\) for \(t\in[0, t_n]\), i.e. all previous times. 

\subsubsection{General Algorithm}
Consider some general data output \(y\), of shape \((N, p_{\text{out}})\) and some data input \(X\), of shape \((N, p_{\text{in}})\), where \(N\) corresponds to the total amount of time points, and \(p_{\text{out}}, \ p_{\text{in}}\) the dimension of the output and input, respectively. Generally, \(X\) could correspond to a quite large sampling frequency in time, making the computation of the hidden state \(h_t\) in \eqref{eq:hidden_state_computation_RNN} computationally demanding. One typical way of dealing with this is to split the data into `windows' of size \(N_{W}\) in time. These windows should generally overlap, such that no temporal dependencies across windows are left out. 

Splitting the data into windows, we define the hidden state for window \(n\) as:
\begin{align}
	h_{n} &= \sigma^{(h)}\left( W_{hx}X_{n} + W_{hh}h_{n-1} + b_{h} \right) \notag \\
	&\equiv \sigma^{(h)}(z_{n})
\end{align}
where \(X_{n}\) is the \(n\)-th window.

\subsubsection{Backpropagation Through Time}
The error between \(y\) and the predicted output \(\tilde{y}\), is given by some chosen loss function \(L(y, \tilde{y})\),
\begin{align}	
	L(y,\tilde{y}) = \frac{1}{N}\sum\limits_{n=1}^{N} l(y_{n}, \tilde{y}_n),
\end{align}
where \(l\) is some error-metric. 
For some learning rate \(\eta\), the standard update rule for the weights and biases is given by:
\begin{subequations}
\begin{align}
	W\leftarrow W - \eta \frac{\partial L}{\partial W},  \ b\leftarrow b - \eta \frac{\partial L}{\partial b}.
\end{align}
\end{subequations}
This transformation may be extended using optimization methods aimed at handling exploding gradient, faster convergence, avoiding local minimas, etc. We covered three of these optimization methods in \cite{project2}; the root mean squared propagation (RMSprop), the adaptive gradient (AdaGrad) and the adaptive moment estimation (Adam). 

Compared to FFNN, computing the gradient of \(L\) with respect to the weights leads to a somewhat more complicated expression. Consider now the partial derivative of the loss function with respect to the weight \(W\),  being either \(W_{hx}\) or \(W_{hh}\) (cf. \cite{superGood}):
\begin{align}
	\frac{\partial L}{\partial W} &= \sum_{n=1}^{N}\frac{\partial L}{\partial \tilde{y}_{n}} \frac{\partial \tilde{y}_{n}}{\partial W}\\
	&= \sum_{n=1}^{N}\frac{\partial L}{\partial \tilde{y}_{n}} \frac{\partial \tilde{y}_{n}}{\partial h_{n}}\frac{\partial h_{n}}{\partial W},
\end{align}
where we can use backpropagation through time (BPTT) to write \cite{superGood}:
\begin{align}
	\frac{\partial L}{\partial W} &= \sum_{n=1}^{N}\sum_{k=1}^{n}\frac{\partial L}{\partial \tilde{y}_{n}} \frac{\partial \tilde{y}_{n}}{\partial h_{n}} \frac{\partial h_{n}}{\partial h_{k}} \frac{\partial h_{k}}{\partial W}.
\end{align}
Notice here that,
\begin{align}
	\frac{\partial h_n}{\partial h_k} &= \frac{\partial h_n}{\partial h_{n-1}} \frac{\partial h_{n-1}}{\partial h_{n-1}} ... \frac{\partial h_{k+1}}{\partial h_{k}} \notag \\
	&= \left( \sigma'_{h}(z_n)W_{hh} \right)\left( \sigma'_{h}(z_{n-1})W_{hh} \right) ... \left( \sigma'_{h}(z_{k+1})W_{hh} \right) \notag\\
	&= \prod_{j=k+1}^{n} \sigma'_{h}(z_{j})W_{hh}.
\end{align}

Define now the errors,
\begin{align}
	\delta^{k}_{hh}\equiv \frac{\partial h_{k}}{\partial W_{hh}}, \ \delta^{k}_{hx}\equiv \frac{\partial h_{k}}{\partial W_{hx}}.
\end{align}
Computing these errors leads to the recursive formula,
\begin{align}
	\delta^{1}_{hh} &= 0, \notag\\
	\delta^{2}_{hh} &= \sigma'_{h}(z_2) h_1,  \notag\\
	\delta^{3}_{hh} &= \sigma'_{h}(z_3) \left( h_2 + W_{hh}\sigma'_{h}(z_2)h_1 \right), \\
	&\dots, \notag \\
	\delta^{k}_{hh} &= \sigma'_{h}(z_{k}) \left( h_{k-1} + W_{hh}\delta^{k-1}_{hh}\right),
\end{align}
with a similar behavior for \(\delta^{N-n}_{hx}\). On the other hand, the last product in the gradient for the (hidden) biases, \(\partial h_{N-n}/\partial b_{h}\), do not lead to a recursive formula (c.f. \cite{superGood}). Hence, for the hidden weights and biases, we have the gradients 
\begin{subequations}
\begin{align}
	\frac{\partial L}{\partial W_{hh}} &= \sum_{n=1}^{N}\sum_{k=1}^{n}\frac{\partial L}{\partial h_{n}}  \left[ \prod_{j=k+1}^{n} \sigma'_{h}(z_{j})W_{hh}\right] \delta^{k}_{hh}, \\
	\frac{\partial L}{\partial W_{hx}} &= \sum_{n=1}^{N}\sum_{k=1}^{n}\frac{\partial L}{\partial h_{n}}  \left[ \prod_{j=k+1}^{n} \sigma'_{h}(z_{j})W_{hh}\right] \delta^{k}_{hx}, \\
	\frac{\partial L}{\partial b_{h}} &= \sum_{n=1}^{N}\sum_{k=1}^{n}\frac{\partial L}{\partial h_{n}}  \left[ \prod_{j=k+1}^{n} \sigma'_{h}(z_{j})W_{hh}\right] \sigma'_{h}(z_k).
\end{align}
\end{subequations}
For the output layer, \(\partial h_n/\partial W_{yh}\) is only non zero of \(n=N\), hence
\begin{subequations}
	\begin{align}
		\frac{\partial L}{\partial W_{yh}} &= \frac{\partial L}{\partial \tilde{y}_{n}} \frac{\partial z_N}{\partial h_{n}} \sigma'_{\text{out}} h_{N}, \\
		\frac{\partial L}{\partial b_{y}} &= \frac{\partial L}{\partial \tilde{y}_{n}} \frac{\partial z_N}{\partial h_{n}} \sigma'_{\text{out}}.
	\end{align}
\end{subequations}
\begin{align}
	\frac{\partial L}{\partial W_{yh}} &= \frac{\partial L}{\partial h_{N}}  \left[ \prod_{j=k+1}^{n} \sigma'_{h}(z_{j})W_{hh}\right] \sigma'_{\text{out}}(z_N)h_N, \\
	\frac{\partial L}{\partial b_{h}} &= \frac{\partial L}{\partial h_{N}}  \left[ \prod_{j=k+1}^{n} \sigma'_{h}(z_{j})W_{hh}\right] \sigma'_{\text{out}}(z_N).
\end{align}

The dependency for each error term \(\delta^{N-n}\) on `past' error terms leads to a much greater computation time, compared to that of FFNN. Every gradient computation need an additional propagation through all time-windows. This can lead to gradients blowing up due to only (relatively) minor errors. However, there are multiple ways of resolving this issue. Perhaps the most obvious one is to simply truncate the amount of terms in the algorithm, commonly referred to as `truncated backpropagation through time' (see e.g. \cite{tallec2017unbiasingtruncatedbackpropagationtime}). Apart from that it is an actual simplification, it has the immediate consequence of ignoring long-term dependencies of the data, which in some cases is just the type of information you do not want your model to train on. 




Implementing the stacked RNN is then done by essentially creating a hidden state for each `stack' of RNN. The output of the stacked RNN is computed by feeding the hidden states to each other in succession, starting from the first hidden layer. The hidden states in some time window \(n\) are given by
\begin{align}
	h^{l}_{n} = \begin{cases}
		\sigma^{(h)}\left( W_{hx}^{1}X_{n} + W_{hh}^{1}h_{n-1}^{1} + b_{h}^{1}\right), &l=1, \\
		\sigma^{(h)}\left( W_{hx}^{l}h_{n}^{l-1} + W_{hh}^{l}h_{n-1}^{l} + b_{h}^{l}\right), &l\geq 2,
	\end{cases}
\end{align}	
and the output of the stacked RNN in time window \(n\) as
\begin{align}
	\tilde{y}_{n} &= \sigma^{(\text{out})}\left( W_{yh}h^{L}_{n} + b_{y} \right).
\end{align}
Here, the dimensions are \(W^{l}_{hx}\in\mathbb{R}^{d_{l}\times d_{l-1}}\), \(W^{l}_{hh}\in\mathbb{R}^{d_{l}\times d_{l}}\), with \(d_l\) being the dimension of the \(l\)-th hidden state, \(l_0\) the dimension of the input and \(l^{L}\) the dimension of the output. The BTT algorithm for a stacked RNN takes on the same form, except that we now have \(L\) hidden states. 

\subsubsection{Gradient Clipping}
A common method for dealing with exploding gradients, is the method of gradient clipping (see e.g. \cite{Goodfellow-et-al-2016}). This method prevents checks whether the magnitude of the gradient is moving past a certain threshold. If this is true it truncates the current gradient. This can be summarized as:
\begin{align}
	\nabla L \rightarrow \frac{\epsilon}{||\nabla L||}\nabla L \text{ if } ||\nabla L|| > \epsilon.
\end{align}

\section{Implementation}


\section{Discussion}

\section{Conclusion}
Test bib \cite{Planck:2018vyg}

% Bibliography
\bibliographystyle{JHEP}
\bibliography{project3}
	
\end{document}