\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{USGS_EarthExplorer}
\citation{Bishop2006}
\citation{USGS_EarthExplorer}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Project 1}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory}{1}{section*.4}\protected@file@percent }
\newlabel{eq:data}{{1}{1}{}{equation.2.1}{}}
\newlabel{eq:model}{{2}{1}{}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}OLS}{2}{section*.5}\protected@file@percent }
\newlabel{sec:THEORY_OLS}{{2\,2.1}{2}{}{section*.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Ridge}{2}{section*.6}\protected@file@percent }
\newlabel{sec:THEORY_Ridge}{{2\,2.2}{2}{}{section*.6}{}}
\newlabel{eq:cost_ridge}{{7}{2}{}{equation.2.7}{}}
\newlabel{eq:beta_ridge}{{8}{2}{}{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}LASSO}{2}{section*.7}\protected@file@percent }
\newlabel{sec:THEORY_LASSO}{{2\,2.3}{2}{}{section*.7}{}}
\newlabel{eq:cost_lasso}{{9}{2}{}{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Connection to Statistics}{2}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Bias-Variance}{3}{section*.9}\protected@file@percent }
\newlabel{eq:MSE-bias-var}{{16}{3}{}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Resampling}{3}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Bootstrap}{3}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Cross-Validation}{4}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{4}{section*.13}\protected@file@percent }
\newlabel{eq:franke}{{17}{4}{}{equation.3.17}{}}
\newlabel{eq:bmz}{{18}{4}{}{equation.3.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{5}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The Franke Function}{5}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}OLS}{5}{section*.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The (log) MSE of the train (solid) and test (dashed) data from the OLS regression method using data generated by \(N=100\) datapoints from the Franke function in \eqref  {eq:franke}, with noise proportional to \(\epsilon =0.1\). The y-axis is logarithmic, and the x-axis gives the polynomial degree \(p\in [0,17]\). No scaling has been applied to the data.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:OLS_mse_degree}{{1}{5}{The (log) MSE of the train (solid) and test (dashed) data from the OLS regression method using data generated by \(N=100\) datapoints from the Franke function in \eqref {eq:franke}, with noise proportional to \(\epsilon =0.1\). The y-axis is logarithmic, and the x-axis gives the polynomial degree \(p\in [0,17]\). No scaling has been applied to the data}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The \(R^2\)-score of train (solid) and test (dashed) data from the OLS regression method using data generated by \(N=100\) datapoints from the Franke function in \eqref  {eq:franke}, with noise proportional to \(\epsilon =0.1\). The x-axis gives the polynomial degree \(p\in [0,17]\). No scaling has been applied to the data.}}{5}{figure.2}\protected@file@percent }
\newlabel{fig:OLS_r2_degree}{{2}{5}{The \(R^2\)-score of train (solid) and test (dashed) data from the OLS regression method using data generated by \(N=100\) datapoints from the Franke function in \eqref {eq:franke}, with noise proportional to \(\epsilon =0.1\). The x-axis gives the polynomial degree \(p\in [0,17]\). No scaling has been applied to the data}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The \(\beta \)-coefficients for the OLS-fit of \(N=100\) datapoints generated from the Franke function \eqref  {eq:franke} with noise proportional to \(\epsilon =0.1\). The plots shows the polynomial degree from 0 (green) to 17 (red). The left hand side is restricted to \(\beta \)-coefficients corresponding to up to \(p=8\). The solid lines in the background indicate polynomial degree. No scaling has been applied to the data.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:OLS_beta_degree_100}{{3}{5}{The \(\beta \)-coefficients for the OLS-fit of \(N=100\) datapoints generated from the Franke function \eqref {eq:franke} with noise proportional to \(\epsilon =0.1\). The plots shows the polynomial degree from 0 (green) to 17 (red). The left hand side is restricted to \(\beta \)-coefficients corresponding to up to \(p=8\). The solid lines in the background indicate polynomial degree. No scaling has been applied to the data}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The \(\beta \)-coefficients for the OLS-fit of \(N=1000\) datapoints generated from the Franke function \eqref  {eq:franke} with noise proportional to \(\epsilon =0.1\). The plots shows the polynomial degree from 0 (green) to 17 (red). The left hand side is restricted to \(\beta \)-coefficients corresponding to up to \(p=8\). The solid lines in the background indicate polynomial degree. No scaling has been applied to the data.}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:OLS_beta_degree_1000}{{4}{6}{The \(\beta \)-coefficients for the OLS-fit of \(N=1000\) datapoints generated from the Franke function \eqref {eq:franke} with noise proportional to \(\epsilon =0.1\). The plots shows the polynomial degree from 0 (green) to 17 (red). The left hand side is restricted to \(\beta \)-coefficients corresponding to up to \(p=8\). The solid lines in the background indicate polynomial degree. No scaling has been applied to the data}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The MSE of train (solid) and test (dashed) data from the OLS regression method using data generated by \(N=100\) datapoints from the Franke function in \eqref  {eq:franke}, with noise proportional to \(\epsilon =0.1\). The y-axis is logarithmic, and the x-axis gives the polynomial degree \(p\in [0,17]\). Min-max scaling has been applied to the data.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:OLS_mse_degree_minmax}{{5}{6}{The MSE of train (solid) and test (dashed) data from the OLS regression method using data generated by \(N=100\) datapoints from the Franke function in \eqref {eq:franke}, with noise proportional to \(\epsilon =0.1\). The y-axis is logarithmic, and the x-axis gives the polynomial degree \(p\in [0,17]\). Min-max scaling has been applied to the data}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The MSE of train (solid) and test (dashed) data from the OLS regression method using data generated by \(N=100\) datapoints from the Franke function in \eqref  {eq:franke}, with noise proportional to \(\epsilon =0.1\). The y-axis is logarithmic, and the x-axis gives the polynomial degree \(p\in [0,17]\). Standard scaling has been applied to the data.}}{6}{figure.6}\protected@file@percent }
\newlabel{fig:OLS_mse_degree_standard}{{6}{6}{The MSE of train (solid) and test (dashed) data from the OLS regression method using data generated by \(N=100\) datapoints from the Franke function in \eqref {eq:franke}, with noise proportional to \(\epsilon =0.1\). The y-axis is logarithmic, and the x-axis gives the polynomial degree \(p\in [0,17]\). Standard scaling has been applied to the data}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Ridge}{6}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Ridge MSE as a function of the max polynomial degree with \(N=50\), \(p_{\text  {max}}=10\) and \(\epsilon =0.1\). The four chosen values for the hyperparameter $\lambda $ are $10^{-15},10^{-7},10^{-4}$ and $10^{-1}$ which are denoted by the different colors. The solid and dashed lines correspond to the training and test set respectively.}}{6}{figure.7}\protected@file@percent }
\newlabel{fig:ridge_mse_degree}{{7}{6}{Ridge MSE as a function of the max polynomial degree with \(N=50\), \(p_{\text {max}}=10\) and \(\epsilon =0.1\). The four chosen values for the hyperparameter $\lambda $ are $10^{-15},10^{-7},10^{-4}$ and $10^{-1}$ which are denoted by the different colors. The solid and dashed lines correspond to the training and test set respectively}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Ridge $R^2$-score as a function of the max polynomial degree with \(N=50\), \(p_{\text  {max}}=10\) and \(\epsilon =0.1\). The line and color scheme is the same as for Fig. \ref {fig:ridge_mse_degree}.}}{7}{figure.8}\protected@file@percent }
\newlabel{fig:ridge_r2_degree}{{8}{7}{Ridge $R^2$-score as a function of the max polynomial degree with \(N=50\), \(p_{\text {max}}=10\) and \(\epsilon =0.1\). The line and color scheme is the same as for Fig. \ref {fig:ridge_mse_degree}}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Ridge MSE with max polynomial degree $4$ as a function of the hyperparameter $\lambda $ with $N=50$ and $\epsilon =0.1$.}}{7}{figure.9}\protected@file@percent }
\newlabel{fig:ridge_logMSE_degree}{{9}{7}{Ridge MSE with max polynomial degree $4$ as a function of the hyperparameter $\lambda $ with $N=50$ and $\epsilon =0.1$}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Ridge $R^2$-score with max polynomial degree $4$ as a function of the hyperparameter $\lambda $ with $N=50$ and $\epsilon =0.1$.}}{7}{figure.10}\protected@file@percent }
\newlabel{fig:ridge_logR2_degree}{{10}{7}{Ridge $R^2$-score with max polynomial degree $4$ as a function of the hyperparameter $\lambda $ with $N=50$ and $\epsilon =0.1$}{figure.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}LASSO}{7}{section*.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Ridge MSE as a function of the max polynomial degree with \(N=50\), \(p_{\text  {max}}=10\) and \(\epsilon =0.1\). The three chosen values for the hyperparameter $\lambda $ are $10^{-10},10^{-2}$ and $10^{-1}$ which follow the same scheme as Fig. \ref {fig:ridge_mse_degree}}}{7}{figure.11}\protected@file@percent }
\newlabel{fig:lasso_mse_degree}{{11}{7}{Ridge MSE as a function of the max polynomial degree with \(N=50\), \(p_{\text {max}}=10\) and \(\epsilon =0.1\). The three chosen values for the hyperparameter $\lambda $ are $10^{-10},10^{-2}$ and $10^{-1}$ which follow the same scheme as Fig. \ref {fig:ridge_mse_degree}}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Caption}}{7}{figure.12}\protected@file@percent }
\newlabel{fig:lasso_r2_degree}{{12}{7}{Caption}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces LASSO MSE with max polynomial degree $4$ as a function of the hyperparameter $\lambda $ with $N=50$ and $\epsilon =0.1$.}}{7}{figure.13}\protected@file@percent }
\newlabel{fig:LASSO_logMSE_degree}{{13}{7}{LASSO MSE with max polynomial degree $4$ as a function of the hyperparameter $\lambda $ with $N=50$ and $\epsilon =0.1$}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces LASSO $R^2$-score with max polynomial degree $4$ as a function of the hyperparameter $\lambda $ with $N=50$ and $\epsilon =0.1$.}}{8}{figure.14}\protected@file@percent }
\newlabel{fig:LASSO_logR2_degree}{{14}{8}{LASSO $R^2$-score with max polynomial degree $4$ as a function of the hyperparameter $\lambda $ with $N=50$ and $\epsilon =0.1$}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Cross Validation and Bootstrap}{8}{section*.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Cross validation for the three models OLS, Ridge and LASSO from data generated from \(N=100\) samples of the Franke function \eqref  {eq:franke} with noise proportional to \(\epsilon =0.1\). The y-axis shows the (logarithmic) MSE with errorbars for the three methods, with \(\lambda =10^{-10}\). }}{8}{figure.15}\protected@file@percent }
\newlabel{fig:CV_0}{{15}{8}{Cross validation for the three models OLS, Ridge and LASSO from data generated from \(N=100\) samples of the Franke function \eqref {eq:franke} with noise proportional to \(\epsilon =0.1\). The y-axis shows the (logarithmic) MSE with errorbars for the three methods, with \(\lambda =10^{-10}\)}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Cross validation for the three models OLS, Ridge and LASSO from data generated from \(N=100\) samples of the Franke function \eqref  {eq:franke} with noise proportional to \(\epsilon =0.1\). The y-axis shows the (logarithmic) MSE with errorbars for the three methods, with \(\lambda =10^{-7}\). }}{8}{figure.16}\protected@file@percent }
\newlabel{fig:CV_1}{{16}{8}{Cross validation for the three models OLS, Ridge and LASSO from data generated from \(N=100\) samples of the Franke function \eqref {eq:franke} with noise proportional to \(\epsilon =0.1\). The y-axis shows the (logarithmic) MSE with errorbars for the three methods, with \(\lambda =10^{-7}\)}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Cross validation for the three models OLS, Ridge and LASSO from data generated from \(N=100\) samples of the Franke function \eqref  {eq:franke} with noise proportional to \(\epsilon =0.1\). The y-axis shows the (logarithmic) MSE with errorbars for the three methods, with \(\lambda =10^{-4}\). }}{8}{figure.17}\protected@file@percent }
\newlabel{fig:CV_2}{{17}{8}{Cross validation for the three models OLS, Ridge and LASSO from data generated from \(N=100\) samples of the Franke function \eqref {eq:franke} with noise proportional to \(\epsilon =0.1\). The y-axis shows the (logarithmic) MSE with errorbars for the three methods, with \(\lambda =10^{-4}\)}{figure.17}{}}
\bibdata{project1Notes,project1}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Cross validation for the three models OLS, Ridge and LASSO from data generated from \(N=100\) samples of the Franke function \eqref  {eq:franke} with noise proportional to \(\epsilon =0.1\). The y-axis shows the (logarithmic) MSE with errorbars for the three methods, with \(\lambda =10^{-1}\). }}{9}{figure.18}\protected@file@percent }
\newlabel{fig:CV_3}{{18}{9}{Cross validation for the three models OLS, Ridge and LASSO from data generated from \(N=100\) samples of the Franke function \eqref {eq:franke} with noise proportional to \(\epsilon =0.1\). The y-axis shows the (logarithmic) MSE with errorbars for the three methods, with \(\lambda =10^{-1}\)}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The bias, variance and error of a OLS regression on data generated from \(N=100\) samples of the Franke function \eqref  {eq:franke} with noise proportional to \(\epsilon =0.1\). The metrics are generated using the bootstrap method.}}{9}{figure.19}\protected@file@percent }
\newlabel{fig:CV_0}{{19}{9}{The bias, variance and error of a OLS regression on data generated from \(N=100\) samples of the Franke function \eqref {eq:franke} with noise proportional to \(\epsilon =0.1\). The metrics are generated using the bootstrap method}{figure.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{9}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{9}{section*.21}\protected@file@percent }
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Derivations}{9}{section*.22}\protected@file@percent }
\newlabel{Appendix:Derivations}{{A}{9}{}{section*.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1}Model}{9}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2}Coefficients}{9}{section*.24}\protected@file@percent }
\bibstyle{JHEP}
\@writefile{toc}{\contentsline {subsection}{\numberline {3}Bias-Variance}{10}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}}{10}{section*.26}\protected@file@percent }
\newlabel{LastPage}{{}{10}{}{}{}}
\gdef \@abspage@last{10}
