\documentclass{article}

\usepackage{subfiles}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{float}
\usepackage{mathtools, mathrsfs}
\usepackage{physics}
\usepackage{tcolorbox}
\usepackage{enumerate}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{tikz}
\usepackage{tensor}
\usetikzlibrary{positioning}
\usepackage[compat=1.1.0]{tikz-feynman}
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

\usepackage[%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
]{geometry}

\newcommand{\Hp}{\mathcal{H}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\MS}{\overline{\text{MS}}}
\newcommand{\p}{\partial}
\title{FYS-STK4155 Week 36}
\author{Edvard B. RÃ¸rnes, Isak O. Rukan, Anton A. Brekke}
\begin{document}
	\maketitle
	\section*{Exercise 1}
	Show that
	\begin{align}
		\mathbb{E}[(\bm y-\tilde{\bm y})^2]=\text{Bias}[\tilde{\bm y}]+\text{Var}[\tilde{\bm y}]+\sigma^2
		\label{eq:MSE-bias-var}
	\end{align}
	where $\bm y$ is defined by $\bm y=f(\bm x)+\bm \varepsilon$, $\bm \varepsilon\sim N(0,\sigma^2)$ is a normal distributed error and $f(\bm x)$ is the approximated function given our model $\tilde{\bm y}$ obtained by minimizing $(\bm y-\tilde{\bm y})^2$ with $\tilde{\bm y}=\bm X\bm \beta$. Explain what the terms mean and discuss their interpretations. Perform then a bias-variance analysis of a simple one-dimensional function by studying the MSE value as a function of the complexity of your model. Use OLS only. Discuss the bias and variance trade-off as function of your model complexity (the degree of the polynomial) and the number of data points.\\
	\textbf{Solution:} \\
	For ease of notation we write $f(\bm x)=f$ and simply ignore vector notation since everything is a scalar in the end. Then we have
	\begin{align*}
		\mathbb{E}[(\bm y-\tilde{\bm y})^2]&=\mathbb{E}[(f+\bm\varepsilon-\tilde{\bm y})^2]=\mathbb{E}[(f-\tilde{\bm y})^2]+2\underbrace{\mathbb{E}[(f-\tilde{\bm y})\bm\varepsilon]}_{=\,0}+\underbrace{\mathbb{E}[\bm\varepsilon^2]}_{=\,\sigma^2}\\
		&=\mathbb{E}[((f-\mathbb{E}[\tilde{\bm y}])-(\tilde{\bm y}-\mathbb{E}[\tilde{\bm y}]))^2]+\sigma^2\\
		&=\mathbb{E}[(f-\mathbb{E}[\tilde{\bm y}]))^2]+\mathbb{E}[(\tilde{\bm y}-\mathbb{E}[\tilde{\bm y}])^2]-2\,\mathbb{E}[((f-\mathbb{E}[\tilde{\bm y}])(\tilde{\bm y}-\mathbb{E}[\tilde{\bm y}]))]+\sigma^2\\
		&=\text{Bias}[\tilde{\bm y}]+\text{Var}[\tilde{\bm y}]+\sigma^2-2\,\mathbb{E}[(f-\mathbb{E}[\tilde{\bm y}])(\tilde{\bm y}-\mathbb{E}[\tilde{\bm y}])]
	\end{align*}
	where $\mathbb{E}[(f-\tilde{\bm y})\bm\varepsilon]=0$ is justified by $\bm\varepsilon$ being independent and we note that the wrong definition of the Bias is given in the problem text (with that definition $\sigma^2$ gets put into the `Bias'). All that remains is to show that the last term is 0. Since $\mathbb{E}[f]=f$ and $\mathbb{E}[f\,\mathbb{E}[\tilde{\bm y}]]=f\,\mathbb{E}[\mathbb{E}[\tilde{\bm y}]]=f\,\mathbb{E}[\tilde{\bm y}]$ then
	\begin{align*}
		\mathbb{E}[(f-\mathbb{E}[\tilde y])(\tilde{\bm y}-\mathbb{E}[\tilde y])]&=\mathbb{E}[f\tilde{\bm y}-f\,\mathbb{E}[\tilde{\bm y}]-\tilde{\bm y}\,\mathbb{E}[\tilde y]+\mathbb{E}^2[\tilde{\bm y}]]\\
		&=f\,\mathbb{E}[\tilde{\bm y}]-f\,\mathbb{E}[\tilde{\bm y}]-\mathbb{E}^2[\tilde{\bm y}]+\mathbb{E}^2[\tilde{\bm y}]=0
	\end{align*}
	which proves the claim. 
	
	The LHS of (\ref{eq:MSE-bias-var}) is the expected value of the MSE which tells us how well the model's predictions match the true data on average. The equation shows that we can decompose this expected MSE into 3 different components.
	\begin{itemize}
		\item Bias: This quantity measures how much the model's average prediction differs from its true value. A high bias implies that the model is underfitting the data of is simply too simplistic.
		\item Var: The variance measures how much the model's predictions vary when trained on different datasets. It captures the sensitivity of the model to small changes in the training data. A high variance suggests overfitting, meaning it performs well on the training data but may be capturing noise or false patterns.
		\item $\sigma^2$: This is the irreducible error or noise in the data itself which cannot be explained by the model.
	\end{itemize}
	The idea is to minimize the LHS of (\ref{eq:MSE-bias-var}), so clearly we want to minimize both the bias and the variance at the same time. However these are correlated to one another, so lowering the e.g. the bias will in general increase the variance. So Bias-Variance Tradeoff is essentially trying to optimize the complexity of the model such that we neither overfit nor underfit the model such that it can be generalized to other cases. These quantities can then be used as means to fine tune a model.
	
	Since I have already written it in down I just want to quickly show why (I believe at least) the definition given in the problem text is wrong:
	\begin{align*}
		\mathbb{E}[(\bm y-\tilde{\bm y})^2]&=\mathbb{E}[((\bm y-\mathbb{E}[\tilde y])-(\tilde{\bm y}-\mathbb{E}[\tilde y]))^2]\\
		&=\mathbb{E}[(\bm y-\mathbb{E}[\tilde y])^2]+\mathbb{E}[(\tilde{\bm y}-\mathbb{E}[\tilde y])^2]-2\,\mathbb{E}[(\bm y-\mathbb{E}[\tilde y])(\tilde{\bm y}-\mathbb{E}[\tilde y])]\\
		&\hspace{-2mm}\underbrace{=}_{\text{wrong}}\text{Bias}[\tilde{\bm y}]+\text{Var}[\tilde{\bm y}]-2\,\mathbb{E}[(\bm y-\mathbb{E}[\tilde y])(\tilde{\bm y}-\mathbb{E}[\tilde y])]
	\end{align*}
	Then we have
	\begin{align*}
		\mathbb{E}[(\bm y-\mathbb{E}[\tilde y])(\tilde{\bm y}-\mathbb{E}[\tilde y])]&=\mathbb{E}[(f+\bm\varepsilon-\mathbb{E}[\tilde y])(\tilde{\bm y}-\mathbb{E}[\tilde y])]\\
		&=\mathbb{E}[(f-\mathbb{E}[\tilde y])(\tilde{\bm y}-\mathbb{E}[\tilde y])]+\mathbb{E}[\bm\varepsilon(\tilde{\bm y}-\mathbb{E}[\tilde y])]
	\end{align*}
	I have already shown explicitly that the first term is $0$ and the second term is 0 due to the same reasons as above. So with this definition we would get the wrong result that
	\begin{align*}
		\mathbb{E}[(\bm y-\tilde{\bm y})^2]&=\text{Bias}[\tilde{\bm y}]+\text{Var}[\tilde{\bm y}]
	\end{align*}
	
		
\end{document}