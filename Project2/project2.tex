% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
amsmath,amssymb,
aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{subfiles}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{float}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{physics}
\usepackage{dsfont}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{tensor}
\usepackage{hyperref}% add hypertext capabilities
\usepackage{subcaption}
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\newcommand{\Hp}{\mathcal{H}}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}
\renewcommand{\figurename}{Fig.}
\renewcommand{\tablename}{Table}
\makeatletter
\renewcommand{\subsubsection}{%
	\@startsection
	{subsubsection}%
	{3}%
	{\z@}%
	{.8cm \@plus1ex \@minus .2ex}%
	{.5cm}%
	{\normalfont\small\centering}%
}
\makeatother
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\p}{\partial}


\begin{document}
	
\title{Project 2}
\author{Edvard B. RÃ¸rnes}
\email{e.b.rornes@fys.uio.no}
\author{Isak O. Rukan}
\email{icrukan@uio.no}
\affiliation{Institute of Physics, University of Oslo,\\0371 Oslo,  Norway}
\date{\today}

\begin{abstract}
	In recent years, neural networks have become increasingly important for advancements in various scientific fields. In this report, we develop a program\footnote{\href{https://github.com/EdvardRornes/FYS-STK4155/tree/main/Project1}{Github}} that consists of both linear and logistic regression methods. In particular, the program implements gradient descent, stochastic gradient descent, and a simple feed-forward neural network with backpropagation. The neural network is evaluated against linear regression on the Franke function and against logistic regression using sklearn's breast cancer data, where it identifies malignant tumors. We explore three activation functions: Sigmoid, ReLU, and LeakyReLU. The analysis involves tuning the number of epochs, hidden nodes, and the hyperparameters $\lambda$ (regularization) and $\eta$ (learning rate) to optimal values. On the Franke function, our own implementation of the neural network performed poorly compared to linear regression, with peak $R^2$-scores of $x$ and $y$ respectively. For the cancer data, the highest accuracies achieved with ReLU, LeakyReLU, and Sigmoid activation functions were $97.9\%, 98.6\%$, and $97.9\%$, respectively, compared to $z\%$ for logistic regression. Notably, the Sigmoid activation function exhibited minimal sensitivity to variations in the hyperparameter $\lambda$ at optimal learning rates, whereas ReLU and LeakyReLU showed significant responsiveness. Overall, while LeakyReLU yielded the best performance across all tests, its improvement over the other activation functions was marginal when applied to the cancer dataset. Additionally, the Sigmoid activation function was found to be appreciably slower than the ReLU variants.
\end{abstract}

\maketitle

\section{Introduction}
Over the last few years, machine learning and neural networks have become an increasingly important part of data analysis with an enormous range of applications. From image recognition to predictive analytics and scientific simulations, these techniques are reshaping the way the scientific community tackles complicated problems. The flexibility of neural networks in approximating complex, non-linear relationships has made them indispensable across diverse fields, such as biology, engineering, finance, physics etc. For just a few examples, see \cite{dawid2023modernapplicationsmachinelearning,thapar2023applicationsmachinelearningmodelling,mohammadi2022applicationsmachinelearninghealthcare}.

Neural networks are a powerful tool for handling large datasets where traditional modeling may fall short. This allows us to extract patterns and make accurate predictions. As the applications of these techniques continue to expand, so does the demand for a deeper understanding of their underlying principles and implementation details. Thus, we aim to investigate the foundational aspects of neural networks, explore various activation functions and learning algorithms, and study their effects on model performance. By doing so, we hope to develop a robust framework for applying neural networks effectively to complex real-world data.

The main goal of this project is understand the various techniques behind machines learning, and investigate they can be applied to the healthcare system, specifically in diagnosing malignant breast cancer tumors based on the tumor's several features based on the data from \texttt{sklearn}'s datasets \cite{sklearn}. We do this by testing the performance of a neural network by comparing it with logistic regression as we are working with discrete data. 

We begin by introducing the relevant background necessary to understand the implementations. Then we outline said implementations before discussing the results. The results go over both linear and non-linear regression on the Franke function where performance issues in the implementation are easier to visualize. Finally the neural network and logistic regression are applied to the cancer data. 

\section{Methods}
In this section we present the various methods used in this report. 

\subsection{Linear Regression}
As discussed in a previous project \cite{project1}, linear regression is the simplest method for fitting a continuous given a data set. The data set is approximated by $\bm y=\bm X\bm\beta$ and the $\beta$ coefficients are found by minimizing the cost function. For this project we consider the two regression methods:
\begin{align}
	C_\text{OLS}(\bm\beta)&=\frac{2}{n}(\bm y-\bm X\bm\beta)^2,\\
	C_\text{Ridge}(\bm\beta)&=C_\text{OLS}(\bm\beta)+\lambda||\bm\beta||_2^2.
\end{align}
We then insist that the derivative of these w.r.t. $\bm\beta$ is $0$, and choose the resulting $\beta$ coefficients as out model. Doing this we arrive at:
\begin{align}
	\bm\beta_\text{OLS}&=(\bm X^T\bm X)^{-1}\bm X^T\bm y,\\
	\bm\beta_\text{Ridge}&=(\bm X^T\bm X+\lambda \bm I)^{-1}\bm X^T\bm y.
\end{align}

\subsection{Regularization Terms}
Regularization is a technique to prevent overfitting by adding a penalty to the cost function that discourages complex models. The two common regularization methods that we inspected previously are Ridge and Lasso regularization. In this project we will only be considering Ridge, where the cost function is given by
\begin{align}
	C_\text{Ridge}(\bm \beta) = C_\text{OLS}(\bm \beta) + \lambda \|\bm \beta\|_2^2,
\end{align}
where the hyperparameter $\lambda$ controls the magnitude of the penalty to large coefficients. For more details see \cite{project1}.

\subsection{Logistic Regression}
Whilst linear regression is quite successful in fitting continuous data such as terrain data, when the output is supposed to be discrete it fails. Linear regression predicts values across a continuous spectrum, resulting in predictions outside the range of valid class labels, such as giving negative probabilities. Logistic regression on the other hand is specifically designed for binary classification problems, and is thus ideal when dealing with discrete outcomes. 

Logistic regression models the probability that a given input belongs to a particular class, typically using the sigmoid function to map any real-valued number to a value between 0 and 1. Given an input vector $\bm X$ and weights $\bm\beta$, logistic regression predicts the probability of the class as:
\begin{align}
	P(y=1|\bm X)=\sigma(\bm X\bm\beta)=\frac{1}{1+e^{-\bm X\bm\beta}}
\end{align}
where $\sigma$ represents the sigmoid function. To find optimal weights, logistic regression minimizes the cross-entropy cost function:
\begin{align}
	C(\bm\beta)=-\frac1n\sum_{i=1}^n\left(y_i \ln(\hat y_i)+(1-y_i)\ln(1-\hat y_i) \right)
\end{align}
where $y_i$ is the class label and $\hat{y}_i$ is the predicted probability for sample $i$.

\subsection{Resampling Methods}
Resampling methods are used to estimate the accuracy of predictive models by splitting the data into training and testing sets or by generating multiple datasets. Common techniques include cross-validation and bootstrapping. In cross-validation, the data is split into $k$ folds, and the model is trained on $k-1$ folds and tested on the remaining fold. This process is repeated $k$ times, and the average accuracy is computed. Bootstrapping involves sampling with replacement from the dataset to create multiple training sets. These methods help assess model stability and generalizability on unseen data.

\subsection{Gradient Descent}
Gradient descent (GD) is an essential optimization algorithm in machine learning, commonly used to minimize cost functions by adjusting model parameters iteratively. Given model parameters $\theta$ and a cost function $C(\theta)$, the GD update rule adjusts parameters in the opposite direction of the gradient:
\begin{align}
	\theta_i^{(j+1)}=\theta_i^{(j)}-\eta\pdv{C}{\theta_i}
\end{align}
where $\eta$ is the learning rate. Batch gradient descent (BGD) calculates the gradient over the entire dataset:
\begin{align}
	\theta^{(j+1)}=\theta^{(j)}-\eta\nabla_\theta C
\end{align}
BGD is computationally expensive for large datasets but provides smooth convergence toward the minimum.

\subsection{Stochastic Gradient Descent}
Stochastic Gradient Descent (SGD) is a variation of gradient descent where each parameter update is performed on a single data point or a small batch. The update rule for SGD is:
\[
\theta_i^{(j+1)} = \theta_i^{(j)} - \eta \pdv{C^{(i)}}{\theta_i}
\]
where $C^{(i)}$ is the cost function evaluated at a single data point $i$. While SGD introduces noise in the updates, it often converges faster for large datasets and helps escape local minima, making it ideal for training neural networks.

\subsection{Neural Networks}
Neural networks are computational models inspired by the human brain, designed to recognize patterns and relationships within data. They consist of layers of interconnected neurons or nodes, where each neuron applies a transformation to the input data. In each layer, neurons take a weighted sum of inputs, apply an activation function to introduce non-linearity, and pass the result to the next layer. The final layer produces the output, serving as the networkâs prediction.

\subsubsection{Feed Forward Neural Networks}
Feed-forward neural networks (FFNNs) are the simplest type of neural network, where data flows forward from input to output without forming cycles. These networks contain one or more hidden layers that apply an activation function to capture complex, nonlinear patterns in the data. The training process adjusts the weights of each connection to minimize a cost function, typically using gradient descent.

\subsubsection{Activation Functions}
Activation functions play a critical role in neural networks by introducing non-linearity, which enables the network to approximate more complex functions beyond simple linear mappings. In this project, we use three different activation functions: sigmoid, tanh, ReLU, and Leaky ReLU. Each function has different properties that can impact training performance and convergence.
\begin{itemize}
	\item The sigmoid function, suitable for binary classification, takes an input $z\in\mathbb{R}$ and outputs a value in the range $(0,1)$. This makes it useful for probabilistic interpretations. As mentioned prior, it is given by:
	\begin{align} 
		\sigma(z)=\frac{1}{1+e^{-z}}.
	\end{align}
	\item The ReLU (Rectified Linear Unit) function activates only positive values: 
	\begin{align} 
		R(z)=
		\begin{cases}
			0 & \text{if }z\leq0\\z&\text{if }z>0
		\end{cases}.
	\end{align}
	This reduces the number of calculations that the network has to perform and can speed up the training.
	\item The Leaky ReLU (LReLU) function is a variation of ReLU that allows a small gradient when $z\leq0$. This can help mitigate a known issue known as ``dying ReLU'' where neurons become inactive due to consistently recieving negative inputs., helping to mitigate issues with inactive neurons. It is given by:
	\begin{align} 
		LR(z)=
		\begin{cases} 
			az&\text{if }z\leq0\\z&\text{if }z> 0 
		\end{cases} 
	\end{align}
	where $a$ is some small number. In this project we consider only $a=0.01$.
\end{itemize}
By selecting appropriate activation functions for each layer, FFNNs can effectively capture complex data patterns, enhancing model performance.

\subsubsection{Backpropagation}
Backpropagation is the key algorithm for training neural networks by optimizing weights to minimize the cost function. It works by propagating the error backward from the output layer to the input layers, computing gradients for each weight based on the error. These gradients are then used to update the weights, enabling the network to learn from its errors and make more accurate predictions over time.

\section{Implementation}
(Write about your implementation Isak)

Further, the Feedforward Neural Network (FFNN) was implemented to be able to analyze both the Franke function and the breast cancer data. The FFNN is structured with an input layer, one or more hidden layers, and an output layer, with each layer utilizing an activation function. The architecture is defined by specifying the input size, the number of neurons in hidden layers, and the output size. In the case of the Franke function, the input layer is size 2, whilst for the breast cancer data the input layer corresponds to the number of features, i.e. $30$.

The weights and biases are initialized using random values scaled by the number of input neurons, which aids in faster convergence during training. The network supports ReLU, Sigmoid, and Leaky ReLU as activation functions. 

The forward propagation computes the output of the network by applying the activation function to the weighted sum of inputs at each layer. The backward propagation algorithm updates the weights and biases using gradient descent, where the mean squared error serves as the loss function. We implement regularization techniques to mitigate overfitting.

The training process includes splitting the data into training and test sets, followed by iterating through a specified number of epochs, during which the network adjusts its weights to minimize the error. After training, the network can predict outputs for new data, allowing for evaluation against known values from the Franke function. The overall performance of the model is assessed using MSE and accuracy for Franke and cancer data respectively.


\section{Results \& Discussion}

\subsection{Franke}
The results for the MSE and $R^2$ as a function of the learning rate $\eta$ with 1000 epochs with our own FFNN and keras NN with different activation functions are given in Fig. \ref{fig:NN_Franke_LR_1000}. Here we can see that the MSE ($R^2$) decrease (increase) as the learning rate becomes larger where the maximal learning rate is given by $0.3$. The sigmoid activation can be seen to perform quite poorly here, only achieving a measly $R^2$ score of $\sim0.1$, being slightly better than taking the average. 

Increasing the learning rate any higher than $\eta=0.3$ immediately begins to cause large divergences for multiple of the activation functions. This is on the contrary to what one would expect. If, say $\eta=0.3$ is the optimal learning rate for a given activation function, one would expect $\eta=0.4$ to give a slightly larger MSE, but not completely diverge. It is not clear what could be causing this. The fact that all activation functions also perform best at the same learning rate is also unexpected. Both of these issues are likely a problem with our implementation.
\begin{figure}[ht!]
	\includegraphics[width=0.5\textwidth]{Python/Figures/NN_MSE_R2_Franke_LearningRate_Epochs1000.pdf}
	\caption{MSE and $R^2$ regression results for the FFNN as a function of $\eta$ with 1000 epochs. The results quickly diverge past $0.3$ thus we only plot that far.}
	\label{fig:NN_Franke_LR_1000}
\end{figure}

Further we used the result for the best learning rate to plot the MSE after each epoch, given in Fig. \ref{fig:NN_Franke_Epochs}. The sigmoid activation function seems to flatten quite early on, and already get diminising returns after epoch number 10. ReLU and LReLU both get increasingly better as we increase the number of epochs, flattening towards the end of the plot. Finally the keras version with sigmoid as the activation function has negligible improvements between epochs 10-200, but improves massively after that. It clearly outperforms our implementations throughout. 
\begin{figure}[ht!]
	\includegraphics[width=0.5\textwidth]{Python/Figures/NN_MSE_Franke_Epoch.pdf}
	\caption{The MSE regression results for the FFNN as a function of number of epochs for ReLU, LReLU, Sigmoid and keras.}
	\label{fig:NN_Franke_Epochs}
\end{figure}

This can be seen even clearer in Fig. \ref{fig:3D_Franke}. Here we plot the predictions of each activation function given 250 samples, represented as blue dots, from the Franke function. The sigmoid activation function seemingly only maps the average of all the points. ReLU and LReLU perform appreciably better, but are dwarfed when compared to keras' sigmoid. The latter almost perfectly fits the Franke function given the number of points, in agreement with its much lower MSE. We note here that we did not use any regulator $\lambda$ for any of this data. The reason for this is simply that \texttt{keras} was far too slow to do a complete parameter search.
\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\textwidth]{Python/Figures/NN_3D_Predict_Franke_Epochs1000.pdf}
\caption{3D plots of the regression results for the FFNN with activation functions ReLU, LReLU and Sigmoid, along with keras' neural network using the Sigmoid activation. The blue dots correspond to the sampled points from the Franke function with $250$ total samples. $1000$ epochs were used.}
\label{fig:3D_Franke}
\end{figure}

Next we used the regulator $\lambda$ on the various activation functions for our own FFNN on the Franke function. The 2D parameter plots showing the MSE are given in Fig. \ref{fig:FFNN_Franke_heatmaps}. We can definitely see here that the sigmoid function has much lower variation w.r.t. $\eta$ and $\lambda$, but this also means that it never reaches a good fit either, and instead simply performs roughly as well as taking the average. ReLU and LReLU have much higher dependence on these, and thus one can find that there are parameter combinations which perform much. Though they still perform much worse than linear regression with OLS and Ridge when we compare it to our results from \cite{project1}. But these are ones again beaten our by keras. As mentioned prior, this is likely a mistake of implementation, thus one cannot be so fast to judge the FFNN for these results.
\begin{figure*}[ht!]
	\begin{subfigure}{0.4353\textwidth}
		\includegraphics[width=\textwidth]{Python/Figures/Heatmap_MSE_ReLU_Franke.pdf}
		\caption{ReLU}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.4353\textwidth}
		\includegraphics[width=\textwidth]{Python/Figures/Heatmap_MSE_Leaky ReLU_Franke.pdf}
		\caption{LReLU}
	\end{subfigure}
	\hfill\newline
	\begin{subfigure}{0.4353\textwidth}
		\includegraphics[width=\textwidth]{Python/Figures/Heatmap_MSE_Sigmoid_Franke.pdf}
		\caption{Sigmoid}
	\end{subfigure}
	\caption{MSE for various combinations of $\eta$ and $\lambda$ for ReLU, LReLU and Sigmoid activation functions with $1000$ epochs on the Franke function.}
	\label{fig:FFNN_Franke_heatmaps}
\end{figure*}
\subsection{Cancer Data}
The accuracy of the FFNN for the three different activation functions for different values of $\eta$ and $\lambda$ are given in Fig. \ref{fig:FFNN_cancer_heatmaps}.
\begin{figure*}[ht!]
	\begin{subfigure}{0.4353\textwidth}
		\includegraphics[width=\textwidth]{Python/Figures/Cancer_Accuracy_Heatmap_relu_Epochs100.pdf}
		\caption{ReLU}
		\label{fig:ReLU_heatmap}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.4353\textwidth}
		\includegraphics[width=\textwidth]{Python/Figures/Cancer_Accuracy_Heatmap_lrelu_Epochs100.pdf}
		\caption{LReLU}
		\label{fig:LReLU_heatmap}
	\end{subfigure}
	\hfill\newline
	\begin{subfigure}{0.4353\textwidth}
		\includegraphics[width=\textwidth]{Python/Figures/Cancer_Accuracy_Heatmap_sigmoid_Epochs100.pdf}
		\caption{Sigmoid}
		\label{fig:Sigmoid_heatmap}
	\end{subfigure}
	\caption{Accuracy for various combinations of $\eta$ and $\lambda$ for ReLU, LReLU and Sigmoid activation functions with $1000$ epochs on the breast cancer dataset.}
	\label{fig:FFNN_cancer_heatmaps}
\end{figure*}

\section{Conclusion}
We find that our own implementation of the FFNN is outperformed by simple linear regression, but a properly made FFNN like \texttt{keras} produces an MSE which is an order of magnitude lower than our linear regression code. For logistic regression our FFNN performs quite well, approaching $98\%$ accuracy with certain parameter combinations.

A step forward would for example be testing out different values of $a$ in LReLU and seeing whether it may achieve a higher level of performance.

% Bibliography
\bibliographystyle{JHEP}
\bibliography{project2}
	
\end{document}