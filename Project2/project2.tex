% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
amsmath,amssymb,
aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{subfiles}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{float}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{physics}
\usepackage{dsfont}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{tensor}
\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\newcommand{\Hp}{\mathcal{H}}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}
\renewcommand{\figurename}{Fig.}
\renewcommand{\tablename}{Table}
\makeatletter
\renewcommand{\subsubsection}{%
	\@startsection
	{subsubsection}%
	{3}%
	{\z@}%
	{.8cm \@plus1ex \@minus .2ex}%
	{.5cm}%
	{\normalfont\small\centering}%
}
\makeatother
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\p}{\partial}



\begin{document}
	
\title{Project 2}
\author{Edvard B. RÃ¸rnes}
\email{e.b.rornes@fys.uio.no}
\author{Isak O. Rukan}
\email{Insert Email}
\affiliation{Institute of Physics, University of Oslo,\\0371 Oslo,  Norway}
\date{\today}

\begin{abstract}
	We build a simple neural network to identify malignant tumors using breast cancer data. Different activation functions such as: Sigmoid, $\tanh$, ReLU and LeakyReLU were used. The number of epochs and hidden nodes along with values for the hyper-parameter $\lambda$ and the learning rate $\eta$ were analyzed and tuned to optimal values. The best performing activation function was ... and ...
\end{abstract}

\maketitle

\section{Introduction}
Over the last few years, machine learning and neural networks have become an increasingly important part of data analysis, with a large range of applications. From image recognition to predictive analytics and scientific simulations, these techniques are reshaping the way the scientific community tackles complicated problems. As such, we wish to investigate and gain an understanding of the fundamental principles behind neural networks and how to apply them. 

In this project in particular, we investigate how these techniques can be applied to the healthcare system, specifically in diagnosing malignant breast cancer tumors based on the tumor's several features. We do this by testing the performance of a neural network with logistic regression as we are working with discrete data. The various parameters are optimized using Stochastic Gradient Decent (SGD).

Before dealing with the cancer data, we first start off with comparing X and OLS on the Franke function. Later we attempt to model the Franke function with our own neural network. This is done to ensure that the neural network is correctly implemented.

\section{Theory}
In this section we derive and discuss the relevant theory behind the methods used. 

\subsection{Linear Regression}
As discussed in a previous project (cite project 1), linear regression is the simplest method for fitting a continuous given a data set. The data set is approximated by
\begin{align}
	\bm y=\bm X\bm\beta
\end{align}
and the $\beta$ coefficients are found by minimizing the cost function. For this project we consider the two regression methods
\begin{align}
	C_\text{OLS}(\bm\beta)&=\frac{2}{n}(\bm y-\bm X\bm\beta)^2\\
	C_\text{Ridge}(\bm\beta)&=C_\text{OLS}(\bm\beta)+\lambda||\bm\beta||_2^2
\end{align}
in which one insists that the derivative of these w.r.t. $\bm\beta$ is $0$ and one arrives at
\begin{align}
	\bm\beta_\text{OLS}&=(\bm X^T\bm X)^{-1}\bm X^T\bm y\\
	\bm\beta_\text{Ridge}&=(\bm X^T\bm X+\lambda \bm I)^{-1}\bm X^T\bm y
\end{align}
which is what we will use to compute the coefficients $\beta_i$.

\subsection{Logistic Regression}
Whilst linear regression is quite successful in fitting continuous data such as terrain data, when the output is supposed to be discrete it fails. Linear regression predicts values across a continuous spectrum, resulting in predictions outside the range of valid class labels, such as giving negative probabilities. Logistic regression on the other hand is specifically designed for binary classification problems, and is thus ideal when dealing with discrete outcomes. 

Logistic regression models the probability that a given input belongs to a particular class. It does this by applying a activation function to a linear combination of the input features. For example the sigmoid function 
\begin{align}
	p(z)&=\frac{1}{1+e^{-z}}
\end{align}
maps any $z\in\mathbb{R}$ to a real number in the interval $(0,1)$. Mathematically, logistic regression is
\begin{align}
	P(Y=1|\bm X)&=\frac{1}{1+\exp(\bm X\bm\beta)}
\end{align}
where $P(Y=1|\bm X)$ is the probability that the output $Y$ is $1$ given the input features $\bm X$ and coefficients $\bm \beta$ which describe the model. We then insert a decision boundary for classification, which in case of the sigmoid function is $1/2$. This implies that if $P(Y=1|\bm X)\geq 1/2$ then we say that this instance belongs to class $1$, and otherwise it belongs to class $0$.

The model is then trained on using the Maximum Likelihood Estimation (MLE), which finds the parameters $\bm\beta$ that maximize the likelihood of the observed data. The cost function is logistic regression is then the negative of the log-MLE, i.e.
\begin{align}
	C(\bm\beta)&=-\frac{1}{n}\sum_{i=1}^n[y_i\ln(P(Y=1|\bm X_i))]\\
	&+(1-y_i)\ln(1-P(Y=1|\bm X_i))
\end{align}
The particular activation functions we will use are sigmoid, tanh, ReLU and LeakyReLU shown in (\ref{eq:sigmoid}-\ref{eq:LeakyReLU}) respectively:
\begin{align}
	\label{eq:sigmoid}
	\sigma(z)&=\frac{1}{1+e^{-z}}\\
	\label{eq:tanh}
	\tanh(z)&=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\
	\label{eq:ReLU}
	R(z)&=\begin{cases}
		0 &\qquad\text{if }x\leq0\\
		z &\qquad\text{if }x>0\\
	\end{cases}\\
	\label{eq:LeakyReLU}
	LR(z)&=\begin{cases}
		10^{-2}z&\text{if }x\leq0\\
		z &\text{if }x>0
	\end{cases}
\end{align}

\subsection{Regularization terms}

\subsection{Resampling Methods}

\subsection{Gradient Decent}
Gradient decent (GD) is an essential optimization algorithm in machines learning, and is commonly used to minimize cost functions by adjusting model parameters iteratively. Given model parameters $\theta$ and a cost function $C(\theta)$ the GD algorithm is as follows:
\begin{align}
	\theta_i^{(j+1)}=\theta_i^{(j)}-\eta\pdv{C}{\theta_i}
\end{align}
Taking it a step further we can perform batch gradient decent (BGD)
\begin{align}
	\theta^{(j+1)}&=\theta^{(j)}-\eta\nabla_\theta C
\end{align}

\subsection{Stochastic Gradient Decent}

\subsection{Neural Networks}

\subsubsection{Feed Forward Neural Networks}

\subsubsection{Back Propagation}

\section{Implementation}

\section{Results \& Discussion}

\subsection{Linear Regression}

\subsection{Non-Linear Regression}

\subsection{Logistic Regression}


\section{Conclusion}
Test bib \cite{Planck:2018vyg}

% Bibliography
\bibliographystyle{JHEP}
\bibliography{project2}
	
\end{document}